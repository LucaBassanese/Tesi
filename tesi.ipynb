{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tesi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "27VURv3i7wDI",
        "colab_type": "code",
        "outputId": "2f3b0bbf-5cf5-498a-8600-8e7903231845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install langid\n",
        "!pip install -U sentence-transformers\n",
        "# Commento di prova per testare github"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from langid) (1.17.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-cp36-none-any.whl size=1941190 sha256=cf1e93146490ab20c712e484f9222fdee30138bd6f97577b4d511ccb3193fb6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/bc/61/50a93be85d1afe9436c3dc61f38da8ad7b637a38af4824e86e\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/6e/5c98f5f26698276bacd09077b039fa1a00797ed080a628ee844bd9f281d4/sentence-transformers-0.2.4.1.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hCollecting transformers==2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 14.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.1->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.1->sentence-transformers) (2019.12.9)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.1->sentence-transformers) (1.10.40)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.1->sentence-transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.1->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.1->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.1->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.1->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.1->sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.1->sentence-transformers) (1.13.40)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.1->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.2.1->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers==2.2.1->sentence-transformers) (2.6.1)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.4.1-cp36-none-any.whl size=61094 sha256=3eb3ab896f096f819953d5f1125e072739ee03c92e9f71813bffce5e75853351\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/a5/1c/03b7d87e027121fe1e23048007594e73f39a23e833658529c7\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=d89f0f825cfe24a95943c03e7ddbe93780c4d68cfebdfd6ff78146468971f5ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.4.1 sentencepiece-0.1.85 transformers-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j0tpswpdqVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "8ed21751-fc98-45b3-f398-669d6dd227bc"
      },
      "source": [
        "!curl -O https://raw.githubusercontent.com/LucaBassanese/Tesi/master/data/abstract.xlsx\n",
        "!curl -O https://raw.githubusercontent.com/LucaBassanese/Tesi/master/data/sts-train.csv"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4206k  100 4206k    0     0  13.9M      0 --:--:-- --:--:-- --:--:-- 13.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  880k  100  880k    0     0  3492k      0 --:--:-- --:--:-- --:--:-- 3492k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-FnBKbtrmjL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "406eb597-4cdf-4105-b8e5-73bf8eb36d7c"
      },
      "source": [
        "!git clone https://github.com/LucaBassanese/Tesi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Tesi' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d715KYtvef7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85d700d4-2cef-40fe-a24d-8e8a0ba7388f"
      },
      "source": [
        "import csv\n",
        "data = csv.reader(open('sts-train.csv', encoding=\"utf-8\"),\n",
        "            delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "data"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_csv.reader at 0x7f1bdc42ff98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_aOD4LRhfwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "483549e5-37e1-48f6-db56-6d2bafd724c3"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "df = pd.read_csv('sts-train.csv', encoding='utf-8',\n",
        "                 sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-71af6b19b736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m df = pd.read_csv('sts-train.csv', encoding='utf-8',\n\u001b[0;32m----> 4\u001b[0;31m                  sep='\\t')\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 2508, saw 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XstW9fOg8Bf9",
        "colab_type": "code",
        "outputId": "fbf0b885-c84d-4a29-9301-36dabb1948d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Keywords\n",
        "import sklearn\n",
        "\n",
        "#wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#transalte\n",
        "import langid\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "#cluster of papers\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#cluster hierarchical\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "\n",
        "#Import all the dependencies\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "pd.options.display.max_rows = 150\n",
        "\n",
        "#SBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZlwXK389ljy",
        "colab_type": "text"
      },
      "source": [
        "# Titolo e Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZjxKRGd8KjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url =  'https://raw.githubusercontent.com/LucaBassanese/Tesi/master/data/abstract.xlsx'\n",
        "df = pd.read_excel(url)\n",
        "dft = df.copy()\n",
        "df = df[df['Abstract inglese'].notnull()]\n",
        "df = df.drop_duplicates(['Abstract inglese'])\n",
        "df['titabs']=  df.Titolo + ' ' + df['Abstract inglese']\n",
        "\n",
        "lingua = [langid.classify(testo)[0] for testo in df.Titolo]\n",
        "lingua = pd.Series(lingua)\n",
        "df = df[(lingua == 'en').values]\n",
        "\n",
        "#tokenizer che toglie la punteggiatura\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y9e1p6d6BTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizzo togliendo la punteggiatura\n",
        "words_ta = [tokenizer.tokenize(fr.lower()) for fr in df.titabs] \n",
        "#creo bigrammi e trigrammi\n",
        "bigrams_t = [list(ngrams(word,2)) for word in words_ta ]\n",
        "bigrams_ta = [['_'.join(list(w)) for w in bi] for bi in bigrams_t]\n",
        "trigrams_t = [list(ngrams(word,3)) for word in words_ta ]\n",
        "trigrams_ta = [['_'.join(list(w)) for w in tri] for tri in trigrams_t]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRdlE7uoaz__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creo i tagged document per il doc2vec\n",
        "tagged_data_ta = [TaggedDocument(words=words_ta[i], tags=[str(i)]) for i in range(len(df))]\n",
        "tagged_bi_ta = [TaggedDocument(words=bigrams_ta[i], tags=[str(i)]) for i in range(len(df))]\n",
        "tagged_tri_ta = [TaggedDocument(words=trigrams_ta[i], tags=[str(i)]) for i in range(len(df))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRyaEowZ6_H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohVWsvRQ8pBg",
        "colab_type": "text"
      },
      "source": [
        "# Titoli\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9dX6HIi9sR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dft = dft.drop_duplicates(['Titolo'])\n",
        "# Seleziono sogli i titoli in inglese\n",
        "lingua = [langid.classify(testo)[0] for testo in dft.Titolo]\n",
        "lingua = pd.Series(lingua)\n",
        "dft = dft[(lingua == 'en').values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNYFNpixuB-e",
        "colab_type": "code",
        "outputId": "524aaf15-85af-413f-c8f3-5cc8cf80b2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#tokenizzo togliendo la punteggiatura\n",
        "words = [tokenizer.tokenize(fr.lower()) for fr in dft.Titolo ] \n",
        "#creo bigrammi e trigrammi\n",
        "bigrams_t = [list(ngrams(word,2)) for word in words ]\n",
        "bigrams = [['_'.join(list(w)) for w in bi] for bi in bigrams_t]\n",
        "trigrams_t = [list(ngrams(word,3)) for word in words ]\n",
        "trigrams = [['_'.join(list(w)) for w in tri] for tri in trigrams_t]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdzIuVckRUrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creo i tagged document per il doc2vec\n",
        "tagged_data = [TaggedDocument(words=words[i], tags=[str(i)]) for i in range(len(dft))]\n",
        "tagged_bi = [TaggedDocument(words=bigrams[i], tags=[str(i)]) for i in range(len(dft))]\n",
        "tagged_tri = [TaggedDocument(words=trigrams[i], tags=[str(i)]) for i in range(len(dft))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6F6n2e5NOls",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Frasi unite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGyw4h27NZLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frasi = [' '.join(word) for word in words]\n",
        "frasi_bi = [' '.join(bi) for bi in bigrams]\n",
        "frasi_tri = [' '.join(tri) for tri in trigrams]\n",
        "frasi_ta =  [' '.join(word) for word in words_ta]\n",
        "frasi_bi_ta = [' '.join(bi) for bi in bigrams_ta]\n",
        "frasi_tri_ta = [' '.join(tri) for tri in trigrams_ta]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ioRz6Jy1q2z",
        "colab_type": "text"
      },
      "source": [
        "# Distribuzione parole \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIkNQb5Z2Udi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creo lessico di tutte le parole\n",
        "lessico = tokenizer.tokenize(' '.join(frasi ))\n",
        "stop_words = set(stopwords.words('english')) \n",
        "lessico = [w for w in lessico if not w in stop_words] \n",
        "#creo la distribuzione delle parole nei titoli\n",
        "distr = (pd.Series(lessico)).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5EKHoyI9YnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distr = distr[distr >2] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-jg3eLPHGDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parover95 = list((distr[distr > np.percentile(distr, 95)]).index)\n",
        "par75_80 = list((distr[distr.between(np.percentile(distr, 75), np.percentile(distr, 80))]).index)\n",
        "par50_55 = list((distr[distr.between(np.percentile(distr, 50), np.percentile(distr, 55))]).index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwigGYOZwucF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parindoc_1 = [[d.count(wr) for wr in parover95]for d in frasi]\n",
        "somma_1 = [sum(doc) for doc in parindoc_1]\n",
        "ind_1 = sorted(range(len(somma_1)), key=lambda i: somma_1[i])[-10:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCYOd0e26sYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parindoc_2 = [[d.count(wr) for wr in par75_80]for d in frasi]\n",
        "somma_2 = [sum(doc) for doc in parindoc_2]\n",
        "ind_2 = sorted(range(len(somma_2)), key=lambda i: somma_2[i])[-10:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvpICiPN65kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parindoc_3 = [[d.count(wr) for wr in par50_55]for d in frasi]\n",
        "somma_3 = [sum(doc) for doc in parindoc_3]\n",
        "ind_3 = sorted(range(len(somma_3)), key=lambda i: somma_3[i])[-11:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aUNfk06-MmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indici = (pd.Series(ind_1+ ind_2 + ind_3)).drop_duplicates()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trqLG9S3-iaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indici"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsEDmSikJGCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frasi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFEBtKeuKSPs",
        "colab_type": "text"
      },
      "source": [
        "# SBERT pre training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMsrajGRKbNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKvriouBJeDp",
        "colab_type": "text"
      },
      "source": [
        "# Doc2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHUtWVjSKFIP",
        "colab_type": "code",
        "outputId": "4456c7a0-d690-4486-f5c2-f78411b749b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "max_epochs = 100\n",
        "vec_size = 500\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(vector_size = vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                window = 8,\n",
        "                #dm = 0,\n",
        "                #dbow_words = 1\n",
        "                dm_concat = 1\n",
        "                )\n",
        "  \n",
        "model.build_vocab(tagged_tri_ta)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_tri_ta,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.epochs)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "iteration 1\n",
            "iteration 2\n",
            "iteration 3\n",
            "iteration 4\n",
            "iteration 5\n",
            "iteration 6\n",
            "iteration 7\n",
            "iteration 8\n",
            "iteration 9\n",
            "iteration 10\n",
            "iteration 11\n",
            "iteration 12\n",
            "iteration 13\n",
            "iteration 14\n",
            "iteration 15\n",
            "iteration 16\n",
            "iteration 17\n",
            "iteration 18\n",
            "iteration 19\n",
            "iteration 20\n",
            "iteration 21\n",
            "iteration 22\n",
            "iteration 23\n",
            "iteration 24\n",
            "iteration 25\n",
            "iteration 26\n",
            "iteration 27\n",
            "iteration 28\n",
            "iteration 29\n",
            "iteration 30\n",
            "iteration 31\n",
            "iteration 32\n",
            "iteration 33\n",
            "iteration 34\n",
            "iteration 35\n",
            "iteration 36\n",
            "iteration 37\n",
            "iteration 38\n",
            "iteration 39\n",
            "iteration 40\n",
            "iteration 41\n",
            "iteration 42\n",
            "iteration 43\n",
            "iteration 44\n",
            "iteration 45\n",
            "iteration 46\n",
            "iteration 47\n",
            "iteration 48\n",
            "iteration 49\n",
            "iteration 50\n",
            "iteration 51\n",
            "iteration 52\n",
            "iteration 53\n",
            "iteration 54\n",
            "iteration 55\n",
            "iteration 56\n",
            "iteration 57\n",
            "iteration 58\n",
            "iteration 59\n",
            "iteration 60\n",
            "iteration 61\n",
            "iteration 62\n",
            "iteration 63\n",
            "iteration 64\n",
            "iteration 65\n",
            "iteration 66\n",
            "iteration 67\n",
            "iteration 68\n",
            "iteration 69\n",
            "iteration 70\n",
            "iteration 71\n",
            "iteration 72\n",
            "iteration 73\n",
            "iteration 74\n",
            "iteration 75\n",
            "iteration 76\n",
            "iteration 77\n",
            "iteration 78\n",
            "iteration 79\n",
            "iteration 80\n",
            "iteration 81\n",
            "iteration 82\n",
            "iteration 83\n",
            "iteration 84\n",
            "iteration 85\n",
            "iteration 86\n",
            "iteration 87\n",
            "iteration 88\n",
            "iteration 89\n",
            "iteration 90\n",
            "iteration 91\n",
            "iteration 92\n",
            "iteration 93\n",
            "iteration 94\n",
            "iteration 95\n",
            "iteration 96\n",
            "iteration 97\n",
            "iteration 98\n",
            "iteration 99\n",
            "CPU times: user 1h 26min 2s, sys: 5.59 s, total: 1h 26min 8s\n",
            "Wall time: 44min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5xU8vAfOMe",
        "colab_type": "code",
        "outputId": "99ed8eb1-e162-4cbc-a5d0-d1c94cd54156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wtqKcvzmZGV",
        "colab_type": "code",
        "outputId": "4fc0f59b-9bec-4937-c854-1b725b5fd8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#model_save_name = 'd2v.model'\n",
        "path = F\"/content/gdrive/My Drive/Modelli/d2v.model\" \n",
        "#torch.save(model.state_dict(), path)\n",
        "model.save(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5wYueZHbQNc",
        "colab_type": "code",
        "outputId": "a39b3ecc-7914-4ec6-db36-a96a555551d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "model.save('doc2vec')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTCUP3-Q5tv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = np.zeros((len(dft),400))\n",
        "for i in range(len(dft)):\n",
        "  matrix[i,:] = model.docvecs[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjUqYtjirPJy",
        "colab_type": "text"
      },
      "source": [
        "# SBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpny8Mp3rVfz",
        "colab_type": "code",
        "outputId": "aa935783-5c6c-4b27-dbfa-977dd9968e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sentence_embeddings = model.encode(frasi)\n",
        "sbert = np.stack( sentence_embeddings, axis=0 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:24<00:00, 16.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L97m3oCLSiVi",
        "colab_type": "code",
        "outputId": "4d315159-c20d-4f4d-9eed-9e44e40b7b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.24G/1.24G [00:18<00:00, 67.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2OYn1K4TZjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embeddings = model.encode(frasi)\n",
        "sbert = np.stack( sentence_embeddings, axis=0 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvyHTvD2TkIq",
        "colab_type": "code",
        "outputId": "16ca9a9c-7b61-4fb6-a639-cbfcd225d4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sbert.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1548, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3j97tLArmuR",
        "colab_type": "code",
        "outputId": "0da2c127-aa4e-49e3-b456-413fbe84fec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "sbert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.1434323 , -0.18188004,  0.91623545, ..., -0.48097208,\n",
              "        -0.9856035 ,  0.8776383 ],\n",
              "       [-0.40389788, -0.41126215,  0.42457682, ...,  0.5315726 ,\n",
              "        -0.87461996,  0.44353035],\n",
              "       [-0.24677515, -0.12850131,  0.44034338, ..., -0.7237369 ,\n",
              "        -0.31194127, -0.29219583],\n",
              "       ...,\n",
              "       [-0.3599878 ,  0.2995323 ,  0.62660736, ..., -0.2494359 ,\n",
              "        -0.6457086 ,  0.389404  ],\n",
              "       [-0.49856946,  0.05594404,  0.3238621 , ..., -0.1141041 ,\n",
              "        -0.24745905,  0.60181206],\n",
              "       [ 0.03410498, -0.08580752,  1.8610344 , ..., -1.1958299 ,\n",
              "        -1.1618495 , -0.1159507 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    }
  ]
}